# Pipeline de Procesamiento de Datos - Soluci√≥n Implementada

## Resumen de la Soluci√≥n

Se ha implementado exitosamente un pipeline de procesamiento de datos usando **Azure Data Factory** con validaciones nativas y transformaciones optimizadas. La soluci√≥n utiliza una arquitectura simplificada pero robusta que evita los problemas de cuota de Azure Functions.

## Arquitectura Implementada

### Componentes Principales

1. **Azure Data Factory** - Orquestaci√≥n del pipeline
2. **Azure Data Lake Storage Gen2** - Almacenamiento de datos
3. **Azure SQL Database** - Base de datos transaccional
4. **Azure Key Vault** - Gesti√≥n de secretos
5. **Azure Event Hub** - Ingesta de datos en tiempo real
6. **Log Analytics Workspace** - Monitoreo y logging

### Pipeline Principal: `MainProcessingPipeline`

El pipeline implementado incluye las siguientes actividades:

#### 1. ValidateInputData
- **Tipo**: Validation Activity
- **Funci√≥n**: Valida que los archivos de entrada existan y cumplan criterios m√≠nimos
- **Configuraci√≥n**:
  - Timeout: 5 minutos
  - Tama√±o m√≠nimo: 1 byte
  - Intervalo de verificaci√≥n: 10 segundos

#### 2. CopyAndTransformData
- **Tipo**: Copy Activity
- **Funci√≥n**: Copia y transforma datos de CSV a Parquet
- **Caracter√≠sticas**:
  - Conversi√≥n autom√°tica de tipos
  - Manejo de filas incompatibles
  - Logging de advertencias
  - Optimizaci√≥n con 2 DIUs (Data Integration Units)
  - Comportamiento de copia: FlattenHierarchy

#### 3. LogProcessingMetrics
- **Tipo**: Set Variable Activity
- **Funci√≥n**: Captura m√©tricas de procesamiento
- **M√©tricas registradas**:
  - N√∫mero de filas copiadas
  - N√∫mero de archivos procesados
  - Duraci√≥n del procesamiento

## Configuraci√≥n de Validaciones

### Archivo de Configuraci√≥n: `config/pipeline_config.json`

```json
{
  "validation_rules": {
    "file_validation": {
      "minimum_size_bytes": 1,
      "maximum_size_mb": 100,
      "allowed_extensions": [".csv", ".txt"],
      "encoding": "utf-8"
    },
    "data_validation": {
      "skip_incompatible_rows": true,
      "log_level": "Warning",
      "max_error_percentage": 5,
      "required_columns": ["id", "timestamp", "value"]
    }
  }
}
```

## Estructura de Datos

### Arquitectura Medallion Implementada

```
Storage Account (pdgdevsa001)
‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îî‚îÄ‚îÄ csv/                 # Datos de entrada (Bronze)
‚îú‚îÄ‚îÄ silver/
‚îÇ   ‚îî‚îÄ‚îÄ processed/           # Datos procesados (Silver)
‚îú‚îÄ‚îÄ quarantine/              # Datos con errores
‚îî‚îÄ‚îÄ logs/                    # Logs de procesamiento
```

### Datasets Configurados

1. **CsvSourceDataset**
   - Formato: Delimited Text (CSV)
   - Ubicaci√≥n: `raw/csv/`
   - Encoding: UTF-8

2. **ParquetSinkDataset**
   - Formato: Parquet
   - Ubicaci√≥n: `silver/processed/`
   - Compresi√≥n: Snappy

## Seguridad y Acceso

### Managed Identity
- Data Factory utiliza Managed Identity para acceso seguro
- Permisos configurados para:
  - Storage Blob Data Contributor
  - Key Vault Secrets User
  - SQL DB Contributor

### Key Vault Secrets
- `storage-account-key`: Clave de la cuenta de almacenamiento
- `sql-connection-string`: Cadena de conexi√≥n a SQL Database
- `eventhub-connection-string`: Cadena de conexi√≥n a Event Hub

## Monitoreo y Logging

### Application Insights
- Logging autom√°tico de actividades del pipeline
- M√©tricas de rendimiento
- Alertas configurables

### Log Analytics
- Logs centralizados de todos los componentes
- Consultas KQL para an√°lisis avanzado
- Dashboards de monitoreo

## Pruebas y Validaci√≥n

### Script de Prueba: `scripts/test_pipeline.ps1`

Para ejecutar las pruebas de integraci√≥n:

```powershell
.\scripts\test_pipeline.ps1 `
    -ResourceGroupName "rg-pdg-datapipe-dev-001" `
    -DataFactoryName "pdg-datapipe-dev-df-001" `
    -StorageAccountName "pdgdevsa001"
```

### Funcionalidades del Script de Prueba

1. ‚úÖ Verificaci√≥n de Azure CLI
2. ‚úÖ Creaci√≥n de datos de prueba
3. ‚úÖ Subida de archivos a Storage
4. ‚úÖ Verificaci√≥n de existencia del pipeline
5. ‚úÖ Ejecuci√≥n del pipeline
6. ‚úÖ Monitoreo de la ejecuci√≥n
7. ‚úÖ Verificaci√≥n de resultados
8. ‚úÖ Limpieza de archivos temporales

## Despliegue

### Terraform
La infraestructura se despliega usando Terraform:

```bash
cd terraform
terraform init
terraform plan
terraform apply -auto-approve
```

### Recursos Desplegados
- 1 Resource Group
- 1 Data Factory con pipeline configurado
- 1 Storage Account (ADLS Gen2)
- 1 SQL Server y Database
- 1 Event Hub Namespace
- 1 Key Vault
- 1 Log Analytics Workspace

## Ventajas de la Soluci√≥n Implementada

### ‚úÖ Ventajas

1. **Sin problemas de cuota**: Utiliza servicios nativos de Azure
2. **Costo optimizado**: No requiere Azure Functions o Batch
3. **Escalabilidad**: Data Factory escala autom√°ticamente
4. **Monitoreo integrado**: Logging y m√©tricas nativas
5. **Seguridad**: Managed Identity y Key Vault
6. **Mantenimiento m√≠nimo**: Servicios completamente gestionados

### üîß Caracter√≠sticas T√©cnicas

- **Validaci√≥n de datos**: Actividad de validaci√≥n nativa
- **Transformaci√≥n**: Copy Activity con conversi√≥n de tipos
- **Manejo de errores**: Skip de filas incompatibles
- **Logging**: Registro detallado de m√©tricas
- **Formato optimizado**: Conversi√≥n a Parquet con compresi√≥n

## Pr√≥ximos Pasos

1. **Configurar alertas** en Azure Monitor
2. **Implementar triggers** para ejecuci√≥n autom√°tica
3. **Agregar m√°s validaciones** seg√∫n necesidades del negocio
4. **Configurar CI/CD** para despliegues automatizados
5. **Implementar data lineage** para trazabilidad

## Soporte y Mantenimiento

### Logs y Troubleshooting
- Revisar logs en Azure Data Factory Monitor
- Consultar m√©tricas en Application Insights
- Verificar alertas en Azure Monitor

### Contacto
Para soporte t√©cnico, contactar al equipo de datos de Prodigio.

---

**Fecha de implementaci√≥n**: $(Get-Date -Format "yyyy-MM-dd")  
**Versi√≥n**: 1.0.0  
**Estado**: ‚úÖ Desplegado y funcional