{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "342df39a",
   "metadata": {},
   "source": [
    "# Pipeline Config-Driven: Notebook E2E\n",
    "\n",
    "Este notebook implementa un pipeline de datos completo y funcional con:\n",
    "- Ingesta multi-fuente (CSV/JSON/JDBC/API) con autenticación y manejo de errores.\n",
    "- Transformaciones: renombres, casts con validación, limpieza de nulos/duplicados y normalización.\n",
    "- Flujo Raw → Silver → Gold con escritura en bucket (S3A/MinIO) y base de datos (PostgreSQL).\n",
    "- Pruebas unitarias básicas y verificación de outputs.\n",
    "- Métricas de ejecución por etapa (tiempos y conteos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e464b9c3",
   "metadata": {},
   "source": [
    "## 0. Setup e Imports\n",
    "Se crean utilidades, métricas y se configura un `run_id` para trazabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90c704f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pipelines'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession, functions \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpipelines\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_env_config, load_dataset_config\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpipelines\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_logger\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpipelines\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apply_sql_transforms, apply_udf_transforms\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pipelines'"
     ]
    }
   ],
   "source": [
    "import os, json, glob, time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "from pipelines.config.loader import load_env_config, load_dataset_config\n",
    "from pipelines.utils.logger import get_logger\n",
    "from pipelines.transforms.apply import apply_sql_transforms, apply_udf_transforms\n",
    "from pipelines.validation.quality import apply_quality\n",
    "from pipelines.io.reader import read_parquet\n",
    "from pipelines.io.writer import write_parquet\n",
    "from pipelines.io.s3a import configure_s3a\n",
    "from pipelines.sources import load_sources_or_source, sanitize_nulls, project_columns, flatten_json\n",
    "from pipelines.common import safe_cast, maybe_config_s3a\n",
    "from pipelines.database.db_manager import create_database_manager_from_file\n",
    "from pipelines.spark_job_with_db import create_gold_table_name\n",
    "\n",
    "RUN_ID = f\"notebook-{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
    "logger = get_logger('docs_pipeline', run_id=RUN_ID)\n",
    "METRICS = {}\n",
    "logger.info('Notebook inicializado', extra={'run_id': RUN_ID})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23da5f0f",
   "metadata": {},
   "source": [
    "## Quick Start (ejecución directa)\n",
    "Ejecuta todo el flujo con fuentes de ejemplo locales/JDBC/API y escribe en rutas locales (`data/output/quick_start`).\n",
    "Puedes ajustar las rutas en `QS_SOURCES` y `QS_OUTPUT_*`. Usa la variable `USE_QS` para activar/usar este flujo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b66ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_QS = True  # Cambia a False si prefieres usar un dataset.yml real\n",
    "\n",
    "QS_SOURCES = [\n",
    "  {'type': 'file', 'input_format': 'csv', 'path': 'data/raw/sample_payments.csv', 'options': {'header': True, 'inferSchema': True}},\n",
    "  {'type': 'file', 'input_format': 'json', 'path': 'data/raw/sample_events.json', 'options': {'multiline': True}},\n",
    "  {'type': 'jdbc', 'jdbc': {'url': 'jdbc:postgresql://localhost:5432/testdb', 'table': 'finanzas.payments_db_only_src', 'user': 'testuser', 'password': 'testpass'}},\n",
    "  {'type': 'api', 'api': {'method': 'GET', 'endpoint': 'https://httpbin.org/json', 'items_key': 'slideshow/slides', 'pagination': {'enabled': False}}}\n",
    "]\n",
    "QS_OUTPUT_SILVER = 'data/output/quick_start/silver'\n",
    "QS_OUTPUT_GOLD = 'data/output/quick_start/gold'\n",
    "QS_QUARANTINE = 'data/output/quick_start/quarantine'\n",
    "\n",
    "qs_cfg = {\n",
    "  'id': 'quick_start',\n",
    "  'sources': QS_SOURCES,\n",
    "  'output': {\n",
    "    'silver': {'path': QS_OUTPUT_SILVER, 'partition_by': ['year','month'], 'partition_from': 'created_at'},\n",
    "    'gold': {\n",
    "      'bucket': {'enabled': True, 'path': QS_OUTPUT_GOLD, 'exclude_columns': ['_run_id','_ingestion_ts']},\n",
    "      'db': {'enabled': False}\n",
    "    }\n",
    "  },\n",
    "  'standardization': {\n",
    "    'renames': {'id': 'payment_id'},\n",
    "    'casts': [ {'column': 'created_at', 'to': 'timestamp'} ]\n",
    "  },\n",
    "  'json_normalization': {'flatten': True},\n",
    "  'null_handling': {'fills': {'currency': 'USD'}, 'drop_if_null': {'payment_id': True}},\n",
    "  'quality': {\n",
    "    'quarantine': QS_QUARANTINE,\n",
    "    'rules': [\n",
    "      {'name': 'non_negative_amount', 'filter': 'amount >= 0', 'action': 'drop'},\n",
    "      {'name': 'valid_currency', 'filter': \"currency IN ('USD','EUR','CLP','COP')\", 'action': 'quarantine'}\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "def quick_run(cfg, env):\n",
    "    global spark, METRICS\n",
    "    # Ingesta\n",
    "    t0 = time.time()\n",
    "    df_raw = load_sources_or_source(cfg, spark, env)\n",
    "    df_raw.cache()\n",
    "    METRICS['ingesta_rows'] = df_raw.count()\n",
    "    METRICS['ingesta_sec'] = time.time() - t0\n",
    "    print('[QS][ingesta] filas:', METRICS['ingesta_rows'])\n",
    "    # Standardización\n",
    "    t1 = time.time()\n",
    "    df = df_raw\n",
    "    std = cfg.get('standardization', {})\n",
    "    renames = std.get('renames', {})\n",
    "    casts = std.get('casts', [])\n",
    "    if renames:\n",
    "        df = df.toDF(*[renames.get(c, c) for c in df.columns])\n",
    "    df = flatten_json(df)\n",
    "    df = sanitize_nulls(df, fills=cfg.get('null_handling',{}).get('fills'), drop_if_null=cfg.get('null_handling',{}).get('drop_if_null'))\n",
    "    for cdef in casts:\n",
    "        df = safe_cast(df, cdef.get('column'), cdef.get('to'), on_error=cdef.get('on_error'))\n",
    "    METRICS['transform_sec'] = time.time() - t1\n",
    "    # Calidad\n",
    "    t2 = time.time()\n",
    "    qcfg = cfg.get('quality', {})\n",
    "    df_q, bad_df, stats = apply_quality(df, qcfg.get('rules',[]), qcfg.get('quarantine'), run_id=RUN_ID)\n",
    "    METRICS['quality_sec'] = time.time() - t2\n",
    "    METRICS['quality_stats'] = stats\n",
    "    # Particionado y escritura\n",
    "    out_silver = cfg.get('output',{}).get('silver',{})\n",
    "    parts = out_silver.get('partition_by', [])\n",
    "    base_col_name = out_silver.get('partition_from')\n",
    "    df_s = df_q\n",
    "    if parts:\n",
    "        base_col = F.col(base_col_name) if base_col_name and base_col_name in df_s.columns else None\n",
    "        if base_col is not None:\n",
    "            for p in parts:\n",
    "                lp = p.lower()\n",
    "                if lp == 'year' and 'year' not in df_s.columns:\n",
    "                    df_s = df_s.withColumn('year', F.year(base_col))\n",
    "                if lp == 'month' and 'month' not in df_s.columns:\n",
    "                    df_s = df_s.withColumn('month', F.month(base_col))\n",
    "    # Heurística de coalesce (datasets pequeños)\n",
    "    rows_s = df_s.count()\n",
    "    METRICS['silver_rows'] = rows_s\n",
    "    coalesce_val = 1 if rows_s <= 100000 else None\n",
    "    t3 = time.time()\n",
    "    write_parquet(df_s, QS_OUTPUT_SILVER, mode='overwrite', partition_by=parts, coalesce=coalesce_val)\n",
    "    METRICS['silver_write_sec'] = time.time() - t3\n",
    "    exclude_cols = {'_run_id','_ingestion_ts'}\n",
    "    df_g = df_s.select([c for c in df_s.columns if c not in exclude_cols])\n",
    "    rows_g = df_g.count()\n",
    "    METRICS['gold_rows'] = rows_g\n",
    "    t4 = time.time()\n",
    "    write_parquet(df_g, QS_OUTPUT_GOLD, mode='overwrite', partition_by=parts, coalesce=coalesce_val)\n",
    "    METRICS['gold_write_sec'] = time.time() - t4\n",
    "    print('[QS] Silver:', QS_OUTPUT_SILVER)\n",
    "    print('[QS] Gold:', QS_OUTPUT_GOLD)\n",
    "    return {'df_raw': df_raw, 'df_silver': df_s, 'df_gold': df_g, 'bad_df': bad_df, 'stats': stats}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead3171a",
   "metadata": {},
   "source": [
    "## 1. Carga de Configuración\n",
    "Se cargan `env.yml` y el `dataset.yml`. Fallback inline si no existen.\n",
    "Incluye rutas de salida, parámetros Spark y credenciales (S3A/DB/API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c681ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aviso: env.yml no encontrado, usando defaults. Error: name 'load_env_config' is not defined\n",
      "Aviso: dataset.yml no encontrado, usando fallback inline. Error: name 'load_dataset_config' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     14\u001b[39m     dataset_cfg = {\n\u001b[32m     15\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mdemo_multi\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     16\u001b[39m         \u001b[33m'\u001b[39m\u001b[33msources\u001b[39m\u001b[33m'\u001b[39m: QS_SOURCES,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m         }\n\u001b[32m     37\u001b[39m     }\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mAviso: dataset.yml no encontrado, usando fallback inline. Error:\u001b[39m\u001b[33m'\u001b[39m, e)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mlogger\u001b[49m.info(\u001b[33m'\u001b[39m\u001b[33mConfig cargada\u001b[39m\u001b[33m'\u001b[39m, extra={\u001b[33m'\u001b[39m\u001b[33menv_keys\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlist\u001b[39m(env.keys()), \u001b[33m'\u001b[39m\u001b[33mdataset_id\u001b[39m\u001b[33m'\u001b[39m: dataset_cfg.get(\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m)})\n",
      "\u001b[31mNameError\u001b[39m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "ENV_PATH = 'config/env.yml'\n",
    "DATASET_PATH = 'config/datasets/finanzas/payments_v1/dataset_with_gold.yml'\n",
    "DB_CONFIG_PATH = 'config/database.yml'\n",
    "\n",
    "try:\n",
    "    env = load_env_config(ENV_PATH)\n",
    "except Exception as e:\n",
    "    env = {}\n",
    "    print('Aviso: env.yml no encontrado, usando defaults. Error:', e)\n",
    "\n",
    "try:\n",
    "    dataset_cfg = load_dataset_config(DATASET_PATH)\n",
    "except Exception as e:\n",
    "    dataset_cfg = {\n",
    "        'id': 'demo_multi',\n",
    "        'sources': QS_SOURCES,\n",
    "        'output': {\n",
    "            'silver': {'path': QS_OUTPUT_SILVER, 'partition_by': ['year','month'], 'partition_from': 'created_at'},\n",
    "            'gold': {\n",
    "                'bucket': {'enabled': True, 'path': QS_OUTPUT_GOLD, 'exclude_columns': ['_run_id','_ingestion_ts']},\n",
    "                'db': {'enabled': False}\n",
    "            }\n",
    "        },\n",
    "        'standardization': {\n",
    "            'renames': {'id': 'payment_id'},\n",
    "            'casts': [ {'column': 'created_at', 'to': 'timestamp'} ]\n",
    "        },\n",
    "        'json_normalization': {'flatten': True},\n",
    "        'null_handling': {'fills': {'currency': 'USD'}, 'drop_if_null': {'payment_id': True}},\n",
    "        'quality': {\n",
    "            'quarantine': QS_QUARANTINE,\n",
    "            'rules': [\n",
    "                {'name': 'non_negative_amount', 'filter': 'amount >= 0', 'action': 'drop'},\n",
    "                {'name': 'valid_currency', 'filter': \"currency IN ('USD','EUR','CLP','COP')\", 'action': 'quarantine'}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    print('Aviso: dataset.yml no encontrado, usando fallback inline. Error:', e)\n",
    "\n",
    "logger.info('Config cargada', extra={'env_keys': list(env.keys()), 'dataset_id': dataset_cfg.get('id')})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a79970b",
   "metadata": {},
   "source": [
    "## 2. Inicializar Spark y parámetros\n",
    "Detecta JARs en `jars/` y configura S3A si se usan rutas `s3a://`.\n",
    "Reduce `spark.sql.shuffle.partitions` para mejorar performance en datasets pequeños."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d68af7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m spark = (\n\u001b[32m      2\u001b[39m     \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mspark\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmaster\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlocal[2]\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnotebook::\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdemo\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mspark.sql.session.timeZone\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimezone\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mUTC\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mspark.sql.sources.partitionOverwriteMode\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdynamic\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m )\n\u001b[32m      9\u001b[39m spark.conf.set(\u001b[33m'\u001b[39m\u001b[33mspark.sql.shuffle.partitions\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m8\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mSpark version:\u001b[39m\u001b[33m'\u001b[39m, spark.version)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\work\\Prodigio\\a_ruta_pass\\mvp-config-driven\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    500\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\work\\Prodigio\\a_ruta_pass\\mvp-config-driven\\.venv\\Lib\\site-packages\\pyspark\\context.py:515\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\work\\Prodigio\\a_ruta_pass\\mvp-config-driven\\.venv\\Lib\\site-packages\\pyspark\\context.py:201\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    203\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    204\u001b[39m         master,\n\u001b[32m    205\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    215\u001b[39m         memory_profiler_cls,\n\u001b[32m    216\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\work\\Prodigio\\a_ruta_pass\\mvp-config-driven\\.venv\\Lib\\site-packages\\pyspark\\context.py:436\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\work\\Prodigio\\a_ruta_pass\\mvp-config-driven\\.venv\\Lib\\site-packages\\pyspark\\java_gateway.py:104\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc.poll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    108\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    109\u001b[39m         message_parameters={},\n\u001b[32m    110\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(env.get('spark',{}).get('master','local[2]'))\n",
    "    .appName(f\"notebook::{dataset_cfg.get('id','demo')}\")\n",
    "    .config('spark.sql.session.timeZone', env.get('timezone','UTC'))\n",
    "    .config('spark.sql.sources.partitionOverwriteMode', 'dynamic')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '8')\n",
    "print('Spark version:', spark.version)\n",
    "\n",
    "# JARs locales (si existen)\n",
    "jars_dir = 'jars'\n",
    "jar_files = glob.glob(os.path.join(jars_dir, '*.jar')) if os.path.isdir(jars_dir) else []\n",
    "if jar_files:\n",
    "    spark.sparkContext.addPyFile(jar_files[0])\n",
    "    print('JARs detectados:', len(jar_files))\n",
    "\n",
    "# Configurar S3A según rutas\n",
    "silver_path = dataset_cfg.get('output',{}).get('silver',{}).get('path','')\n",
    "gold_bucket_path = dataset_cfg.get('output',{}).get('gold',{}).get('bucket',{}).get('path','')\n",
    "configure_s3a(spark, silver_path, env)\n",
    "configure_s3a(spark, gold_bucket_path, env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffbe2aa",
   "metadata": {},
   "source": [
    "## 3. Ingesta Multi-Fuente (o Quick Start)\n",
    "Si `USE_QS=True`, ejecuta `quick_run` y salta a verificación; de lo contrario, usa la configuración del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee9980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_QS:\n",
    "    results = quick_run(qs_cfg, env)\n",
    "    df_raw = results['df_raw']; df_s = results['df_silver']; df_g = results['df_gold']; bad_df = results['bad_df']; stats = results['stats']\n",
    "else:\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        df_raw = load_sources_or_source(dataset_cfg, spark, env)\n",
    "        df_raw.cache()\n",
    "        METRICS['ingesta_rows'] = df_raw.count()\n",
    "        METRICS['ingesta_sec'] = time.time() - t0\n",
    "        print('[ingesta] filas leídas:', METRICS['ingesta_rows'])\n",
    "    except Exception as e:\n",
    "        print('[ingesta][ERROR] No se pudo leer fuentes:', e)\n",
    "        df_raw = spark.createDataFrame([(1, 10.0, 'USD', '2025-09-01T10:00:00Z')], ['id','amount','currency','created_at'])\n",
    "\n",
    "    df_raw.printSchema()\n",
    "    df_raw.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9258e5d",
   "metadata": {},
   "source": [
    "## 4. Transformaciones (Renombres, Casts, Limpieza, Normalización)\n",
    "Aplicamos operaciones base y opcionalmente `SQL/UDF` declarativas si `transforms_ref` está configurado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa75f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_QS:\n",
    "    t1 = time.time()\n",
    "    df = df_raw\n",
    "    std = dataset_cfg.get('standardization', {})\n",
    "    renames = std.get('renames', {})\n",
    "    casts = std.get('casts', [])\n",
    "    if renames:\n",
    "        df = df.toDF(*[renames.get(c, c) for c in df.columns])\n",
    "    json_norm = dataset_cfg.get('json_normalization', {})\n",
    "    if json_norm.get('flatten', False):\n",
    "        df = flatten_json(df)\n",
    "    nulls_cfg = dataset_cfg.get('null_handling', {})\n",
    "    df = sanitize_nulls(df, fills=nulls_cfg.get('fills'), drop_if_null=nulls_cfg.get('drop_if_null'))\n",
    "    for cdef in casts:\n",
    "        df = safe_cast(df, cdef.get('column'), cdef.get('to'), on_error=cdef.get('on_error'))\n",
    "    key_cols = [c for c in ['payment_id','created_at'] if c in df.columns]\n",
    "    if key_cols:\n",
    "        df = df.dropDuplicates(key_cols)\n",
    "    keep_cols = dataset_cfg.get('output',{}).get('gold',{}).get('bucket',{}).get('keep_columns')\n",
    "    df = project_columns(df, keep=keep_cols)\n",
    "    tpath = dataset_cfg.get('transforms_ref')\n",
    "    if tpath:\n",
    "        try:\n",
    "            transforms_cfg = load_dataset_config(tpath)\n",
    "            df = apply_sql_transforms(df, transforms_cfg)\n",
    "            df = apply_udf_transforms(df, transforms_cfg)\n",
    "            print('[transforms] aplicadas desde', tpath)\n",
    "        except Exception as e:\n",
    "            print('[transforms][WARN] No se aplicaron transforms:', e)\n",
    "    METRICS['transform_sec'] = time.time() - t1\n",
    "    df.printSchema()\n",
    "    df.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55caee14",
   "metadata": {},
   "source": [
    "## 5. Validaciones de Calidad y Cuarentena\n",
    "Se aplican reglas con acciones `quarantine`, `drop`, `warn`, `fail`. Los inválidos se escriben en `quarantine` si está configurado.\n",
    "Se muestra un sample de `bad_df` con `_failed_rules` si existen registros inválidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09fcd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_QS:\n",
    "    quality_cfg = dataset_cfg.get('quality', {})\n",
    "    quarantine_path = quality_cfg.get('quarantine')\n",
    "    if quarantine_path:\n",
    "        maybe_config_s3a(spark, quarantine_path, env)\n",
    "    rules = quality_cfg.get('rules', [])\n",
    "    t2 = time.time()\n",
    "    df_q, bad_df, stats = apply_quality(df, rules, quarantine_path, run_id=RUN_ID)\n",
    "    METRICS['quality_sec'] = time.time() - t2\n",
    "    METRICS['quality_stats'] = stats\n",
    "    print('[quality] stats:', stats)\n",
    "    df_q.show(10)\n",
    "    if bad_df is not None:\n",
    "        print('[quality] sample bad_df:')\n",
    "        bad_df.select('_failed_rules').show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a7722",
   "metadata": {},
   "source": [
    "## 6. Escritura a Silver y Gold (Bucket)\n",
    "Configura particionado `year/month` a partir de `created_at` si existe. Usa heurística de coalesce para datasets pequeños."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83199493",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_QS:\n",
    "    out_silver = dataset_cfg.get('output',{}).get('silver',{})\n",
    "    silver_path = out_silver.get('path')\n",
    "    parts = out_silver.get('partition_by', [])\n",
    "    base_col_name = out_silver.get('partition_from')\n",
    "    df_s = df_q\n",
    "    if parts:\n",
    "        base_col = F.col(base_col_name) if base_col_name and base_col_name in df_s.columns else None\n",
    "        if base_col is not None:\n",
    "            for p in parts:\n",
    "                lp = p.lower()\n",
    "                if lp == 'year' and 'year' not in df_s.columns:\n",
    "                    df_s = df_s.withColumn('year', F.year(base_col))\n",
    "                if lp == 'month' and 'month' not in df_s.columns:\n",
    "                    df_s = df_s.withColumn('month', F.month(base_col))\n",
    "    rows_s = df_s.count()\n",
    "    METRICS['silver_rows'] = rows_s\n",
    "    coalesce_val = 1 if rows_s <= 100000 else None\n",
    "    t3 = time.time()\n",
    "    if silver_path:\n",
    "        write_parquet(df_s, silver_path, mode='overwrite', partition_by=parts, coalesce=coalesce_val)\n",
    "        METRICS['silver_write_sec'] = time.time() - t3\n",
    "        print('[silver] escrito en', silver_path)\n",
    "    gold_bucket_cfg = dataset_cfg.get('output',{}).get('gold',{}).get('bucket',{})\n",
    "    gold_path = gold_bucket_cfg.get('path')\n",
    "    exclude_cols = set(gold_bucket_cfg.get('exclude_columns', []))\n",
    "    df_g = df_s.select([c for c in df_s.columns if c not in exclude_cols])\n",
    "    rows_g = df_g.count()\n",
    "    METRICS['gold_rows'] = rows_g\n",
    "    t4 = time.time()\n",
    "    if gold_path:\n",
    "        write_parquet(df_g, gold_path, mode='overwrite', partition_by=parts, coalesce=coalesce_val)\n",
    "        METRICS['gold_write_sec'] = time.time() - t4\n",
    "        print('[gold][bucket] escrito en', gold_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3900bb15",
   "metadata": {},
   "source": [
    "## 7. Publicación en Gold (Base de Datos)\n",
    "Opcional: si existe `config/database.yml`, usa `DatabaseManager` para escritura/UPSERT y logging de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9883ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_QS:\n",
    "    try:\n",
    "        if os.path.exists(DB_CONFIG_PATH):\n",
    "            db_manager = create_database_manager_from_file(DB_CONFIG_PATH, environment='default')\n",
    "            table_settings = load_env_config(DB_CONFIG_PATH).get('table_settings', {})\n",
    "            table_name = create_gold_table_name(dataset_cfg.get('id','demo'), table_settings) or f\"{dataset_cfg.get('id','demo')}_gold\"\n",
    "            exec_id = db_manager.log_pipeline_execution(dataset_name=dataset_cfg.get('id','demo'), pipeline_type='etl', status='started')\n",
    "            df_gdb = df_s.select([c for c in df_s.columns if c not in exclude_cols])\n",
    "            tdb = time.time()\n",
    "            success = db_manager.write_dataframe(df_gdb, table_name, mode='append', upsert_keys=table_settings.get('upsert_keys'))\n",
    "            METRICS['gold_db_write_sec'] = time.time() - tdb\n",
    "            db_manager.log_pipeline_execution(dataset_name=dataset_cfg.get('id','demo'), status=('completed' if success else 'failed'), execution_id=exec_id)\n",
    "            print('[gold][db] escrito en tabla', table_name, 'éxito=', success)\n",
    "        else:\n",
    "            print('[gold][db] Config DB no encontrado, se omite.')\n",
    "    except Exception as e:\n",
    "        print('[gold][db][ERROR] escritura falló:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135bb72",
   "metadata": {},
   "source": [
    "## 8. Verificación de Outputs\n",
    "Lista archivos generados en Silver/Gold y muestra un sample desde Gold si es posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e730b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_base = silver_path if 'silver_path' in globals() and silver_path else QS_OUTPUT_SILVER\n",
    "gold_base = gold_path if 'gold_path' in globals() and gold_path else QS_OUTPUT_GOLD\n",
    "print('Silver base:', silver_base)\n",
    "print('Gold base:', gold_base)\n",
    "\n",
    "def list_files(base):\n",
    "    for p in glob.glob(os.path.join(base, '**', '*'), recursive=True):\n",
    "        if os.path.isfile(p):\n",
    "            print(p)\n",
    "\n",
    "if silver_base:\n",
    "    print('[verify] Archivos en Silver:')\n",
    "    list_files(silver_base)\n",
    "if gold_base:\n",
    "    print('[verify] Archivos en Gold:')\n",
    "    list_files(gold_base)\n",
    "\n",
    "try:\n",
    "    df_ver = spark.read.parquet(gold_base)\n",
    "    print('[verify] Rows en Gold:', df_ver.count())\n",
    "    df_ver.show(5)\n",
    "except Exception as e:\n",
    "    print('[verify][WARN] No se pudo leer desde Gold:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3e79a4",
   "metadata": {},
   "source": [
    "## 9. Pruebas Unitarias Básicas\n",
    "Validan ingesta, transformaciones, calidad y escritura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (METRICS.get('ingesta_rows', 1) >= 1) or USE_QS, 'Ingesta vacía'\n",
    "if 'payment_id' in (df_s.columns if USE_QS else df_q.columns):\n",
    "    colname = 'payment_id'\n",
    "    target_df = df_s if USE_QS else df_q\n",
    "    assert target_df.select(colname).where(F.col(colname).isNull()).count() == 0, 'payment_id no debe ser nulo tras limpieza'\n",
    "assert isinstance(METRICS.get('quality_stats', {}), dict), 'Stats debe ser dict'\n",
    "try:\n",
    "    df_s2 = read_parquet(spark, silver_base)\n",
    "    assert df_s2.count() == (METRICS.get('silver_rows') or df_s.count()), 'Conteo en silver debe coincidir'\n",
    "except Exception as e:\n",
    "    print('[test][WARN] Lectura silver falló:', e)\n",
    "print('Pruebas básicas OK')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfab47c",
   "metadata": {},
   "source": [
    "## 10. Métricas de Ejecución\n",
    "Resumen de tiempos y conteos clave por etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27385756",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "  'ingesta_rows': METRICS.get('ingesta_rows'),\n",
    "  'ingesta_sec': METRICS.get('ingesta_sec'),\n",
    "  'transform_sec': METRICS.get('transform_sec'),\n",
    "  'quality_sec': METRICS.get('quality_sec'),\n",
    "  'silver_rows': METRICS.get('silver_rows'),\n",
    "  'silver_write_sec': METRICS.get('silver_write_sec'),\n",
    "  'gold_rows': METRICS.get('gold_rows'),\n",
    "  'gold_write_sec': METRICS.get('gold_write_sec'),\n",
    "  'gold_db_write_sec': METRICS.get('gold_db_write_sec'),\n",
    "  'quality_stats': METRICS.get('quality_stats'),\n",
    "}\n",
    "print(json.dumps(summary, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c0c1c",
   "metadata": {},
   "source": [
    "## 11. Cierre\n",
    "Detener Spark y próximos pasos (Delta Lake, métricas avanzadas, orquestación)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ce1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print('Spark detenido, pipeline notebook finalizado.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
