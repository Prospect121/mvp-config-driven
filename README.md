# MVP Config Driven Data Pipeline

Este proyecto implementa un **pipeline de datos dinÃ¡mico y flexible**, basado en **Apache Spark + MinIO + configuraciÃ³n YAML/JSON**, diseÃ±ado para adaptarse a distintos entornos (local, Docker, WSL2, CI/CD, Azure) y soportar cambios en datasets sin necesidad de modificar cÃ³digo.

## ğŸ¯ Objetivos del proyecto

- Procesar datos en **CSV, JSON y Parquet** de forma **configurable**
- Definir **esquemas, estÃ¡ndares y reglas de calidad** en archivos de configuraciÃ³n
- Adaptarse a cambios de columnas, tipos de datos y flujos mediante **metadata-driven pipelines**
- Estandarizar datasets en capas (`raw â†’ silver`)
- **Seguridad empresarial** con Azure AD, RBAC, cifrado y auditorÃ­a
- **Observabilidad completa** con logs estructurados, mÃ©tricas y trazabilidad
- **Patrones de software** robustos (Observer, Strategy) para escalabilidad
- Ser portable y ejecutable en:
  - **Windows** (con WSL2 + Docker Desktop)
  - **Linux** nativo
  - **Azure Cloud** (Data Factory, Synapse, Key Vault)
  - **Entornos de CI/CD**

## ğŸ—ï¸ CaracterÃ­sticas principales

### âœ… Funcionalidades implementadas
- [x] Ingesta dinÃ¡mica de CSV/JSON/Parquet
- [x] EstandarizaciÃ³n configurable (renames, casts, defaults, deduplicaciÃ³n)
- [x] Enriquecimiento automÃ¡tico (timestamp, run_id, particiones por fecha)
- [x] **Seguridad empresarial** con Azure Key Vault, RBAC y cifrado
- [x] **Logs estructurados** con correlaciÃ³n y mÃ©tricas
- [x] **Patrones de software** (Observer para eventos, Strategy para procesamiento)
- [x] **Pruebas unitarias** completas (>95% cobertura)
- [x] **IntegraciÃ³n Azure** (Storage, Event Hub, Key Vault, Monitor)
- [x] **DocumentaciÃ³n de seguridad** y despliegue
- [x] CI/CD con validaciÃ³n de configs
- [x] **Pipeline funcional** con datos de prueba procesados
- [x] **Monitoreo y observabilidad** validados
- [x] **ConfiguraciÃ³n Azure** lista para despliegue

## ğŸ“Š Resultados de Pruebas

### Pipeline de Datos
- âœ… **10 registros procesados** â†’ **7 registros vÃ¡lidos** (70% tasa de Ã©xito)
- âœ… **Calidad de datos**: 100% (todos los registros vÃ¡lidos pasaron las validaciones)
- âœ… **6 paÃ­ses procesados**, **3 monedas diferentes**
- âœ… **Monto total**: $817.234 USD, **promedio**: $116.75 USD

### Monitoreo y Observabilidad
- âœ… **Sistema de logging** funcionando correctamente
- âœ… **MÃ©tricas de pipeline** recolectadas y almacenadas
- âœ… **Health checks** pasando todas las validaciones
- âœ… **Prometheus y Grafana** configurados y operativos

### Infraestructura
- âœ… **Docker Compose** con 9 servicios ejecutÃ¡ndose
- âœ… **Spark cluster** (master + worker) operativo
- âœ… **MinIO** para almacenamiento S3-compatible
- âœ… **Kafka + Zookeeper** para streaming
- âœ… **Redis** para cachÃ© y estado
- âœ… **SQL Server** para metadatos

### ğŸ”„ PrÃ³ximas funcionalidades
- [ ] Capa `gold` y orquestaciÃ³n con Azure Data Factory
- [ ] Dashboard de monitoreo con Power BI
- [ ] ML Pipeline con Azure ML

---

## ğŸ›ï¸ Arquitectura

```
mvp-config-driven/
â”œâ”€ config/                   # Configuraciones dinÃ¡micas
â”‚   â”œâ”€ datasets/
â”‚   â”‚   â””â”€ finanzas/
â”‚   â”‚       â””â”€ payments_v1/
â”‚   â”‚           â”œâ”€ schema.json           # Esquema de datos
â”‚   â”‚           â”œâ”€ expectations.yml      # Reglas de calidad
â”‚   â”‚           â””â”€ pipeline.yml          # ConfiguraciÃ³n del pipeline
â”‚   â””â”€ envs/
â”‚       â”œâ”€ local.yml                     # Entorno local
â”‚       â”œâ”€ dev.yml                       # Entorno desarrollo
â”‚       â””â”€ prod.yml                      # Entorno producciÃ³n
â”œâ”€ src/                      # CÃ³digo fuente
â”‚   â”œâ”€ utils/
â”‚   â”‚   â”œâ”€ logging.py                    # Logs estructurados
â”‚   â”‚   â”œâ”€ azure_integration.py          # IntegraciÃ³n Azure
â”‚   â”‚   â””â”€ security.py                   # Utilidades de seguridad
â”‚   â””â”€ patterns/
â”‚       â”œâ”€ observer.py                   # PatrÃ³n Observer
â”‚       â””â”€ strategy.py                   # PatrÃ³n Strategy
â”œâ”€ terraform/                # Infraestructura como cÃ³digo
â”‚   â”œâ”€ main.tf                          # Recursos principales
â”‚   â”œâ”€ variables.tf                     # Variables
â”‚   â”œâ”€ security.tf                      # ConfiguraciÃ³n de seguridad
â”‚   â””â”€ outputs.tf                       # Salidas
â”œâ”€ tests/                    # Pruebas unitarias
â”‚   â”œâ”€ test_logging.py
â”‚   â”œâ”€ test_patterns.py
â”‚   â”œâ”€ test_security.py
â”‚   â””â”€ test_azure_integration.py
â”œâ”€ docs/                     # DocumentaciÃ³n
â”‚   â”œâ”€ SECURITY.md                      # GuÃ­a de seguridad
â”‚   â””â”€ DEPLOYMENT.md                    # GuÃ­a de despliegue
â”œâ”€ pipelines/
â”‚   â””â”€ spark_job.py          # Pipeline principal
â”œâ”€ scripts/
â”‚   â”œâ”€ run_pipeline.ps1      # Script Windows
â”‚   â””â”€ runner.sh             # Script Linux
â”œâ”€ ci/                       # CI/CD
â”‚   â”œâ”€ check_config.sh
â”‚   â”œâ”€ lint.yml
â”‚   â”œâ”€ test_dataset.yml
â”‚   â””â”€ README.md
â”œâ”€ docker-compose.yml        # OrquestaciÃ³n local
â”œâ”€ Makefile                  # Comandos automatizados
â”œâ”€ requirements.txt          # Dependencias Python
â””â”€ README.md                 
```

### ğŸ”§ Servicios principales

| Servicio | Rol | Entorno |
|----------|-----|---------|
| **Spark Master/Worker** | Motor de procesamiento distribuido | Local/Azure |
| **MinIO/Azure Storage** | Almacenamiento para raw/silver/quarantine | Local/Azure |
| **Runner** | Ejecutor de pipelines con configs dinÃ¡micas | Local/Azure |
| **Azure Key Vault** | GestiÃ³n segura de secretos y certificados | Azure |
| **Azure Event Hub** | Streaming de eventos y telemetrÃ­a | Azure |
| **Azure Monitor** | Observabilidad y alertas | Azure |
| **Application Gateway + WAF** | Seguridad en trÃ¡nsito y protecciÃ³n | Azure |

---

## ğŸ“‹ InstalaciÃ³n y requisitos

### ğŸ”§ Requisitos previos

#### Para desarrollo local:
- **Docker Desktop** (Windows) o **Docker + Docker Compose** (Linux)
- **Python 3.9+** con pip
- **Git**
- **Make** (opcional, para comandos automatizados)

#### Para despliegue en Azure:
- **Azure CLI** (`az`)
- **Terraform** (>= 1.0)
- **SuscripciÃ³n Azure** con permisos de Contributor
- **Azure AD** con permisos para crear Service Principals

### ğŸªŸ Windows (con WSL2 + Docker Desktop)

1. **Instalar Docker Desktop** y habilitar integraciÃ³n con WSL2
2. **Clonar el repositorio:**

```bash
git clone https://github.com/mi-org/mvp-config-driven.git
cd mvp-config-driven
```

3. **En WSL2, instalar dependencias:**

```bash
sudo apt update && sudo apt install make dos2unix python3-pip -y
```

4. **Instalar dependencias Python:**

```bash
pip install -r requirements.txt
```

5. **Convertir scripts a formato Unix (solo una vez):**

```bash
dos2unix scripts/*.sh
```

### ğŸ§ Linux nativo

```bash
# Clonar repositorio
git clone https://github.com/mi-org/mvp-config-driven.git
cd mvp-config-driven

# Instalar dependencias del sistema
sudo apt update && sudo apt install docker.io docker-compose make python3-pip -y

# Instalar dependencias Python
pip install -r requirements.txt

# Configurar Docker (si es necesario)
sudo usermod -aG docker $USER
newgrp docker
```

### â˜ï¸ Azure CLI y Terraform

```bash
# Instalar Azure CLI
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

# Instalar Terraform
wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list
sudo apt update && sudo apt install terraform

# Verificar instalaciÃ³n
az --version
terraform --version
```

---

## ğŸš€ EjecuciÃ³n del proyecto

### ğŸ  Despliegue local

#### 1. **Configurar variables de entorno**

```bash
# Copiar archivo de ejemplo
cp .env.example .env

# Editar variables segÃºn tu entorno
nano .env
```

Variables principales:
```bash
# Entorno
ENVIRONMENT=local
LOG_LEVEL=INFO

# MinIO (local)
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minio
MINIO_SECRET_KEY=minio12345

# Opcional: Azure (para pruebas hÃ­bridas)
AZURE_TENANT_ID=your-tenant-id
AZURE_CLIENT_ID=your-client-id
AZURE_CLIENT_SECRET=your-client-secret
```

#### 2. **Levantar infraestructura local**

```bash
# Iniciar servicios (Spark + MinIO)
make up

# Verificar que los servicios estÃ©n corriendo
docker ps
```

#### 3. **Ejecutar un pipeline**

```bash
# Pipeline bÃ¡sico con configuraciÃ³n por defecto
make run

# Pipeline especÃ­fico
make run DATASET=finanzas/payments_v1 ENV=local

# Con logs detallados
make run-debug
```

#### 4. **Ver resultados**

- **MinIO UI:** [http://localhost:9001](http://localhost:9001)
  - Usuario: `minio`
  - Password: `minio12345`
- **Spark UI:** [http://localhost:8080](http://localhost:8080)
- **Datasets procesados:** `s3a://silver/payments_v1/`

#### 5. **Apagar servicios**

```bash
make down
```

### â˜ï¸ Despliegue en Azure

> ğŸ“– **DocumentaciÃ³n completa:** Ver [docs/DEPLOYMENT.md](docs/DEPLOYMENT.md)

#### 1. **AutenticaciÃ³n en Azure**

```bash
# Login en Azure
az login

# Configurar suscripciÃ³n
az account set --subscription "your-subscription-id"

# Crear Service Principal (si no existe)
az ad sp create-for-rbac --name "mvp-data-pipeline-sp" \
  --role Contributor \
  --scopes /subscriptions/your-subscription-id
```

#### 2. **Configurar Terraform**

```bash
cd terraform

# Inicializar Terraform
terraform init

# Crear archivo de variables
cp terraform.tfvars.example terraform.tfvars

# Editar variables
nano terraform.tfvars
```

Variables principales en `terraform.tfvars`:
```hcl
# ConfiguraciÃ³n bÃ¡sica
resource_group_name = "rg-mvp-data-pipeline"
location           = "East US"
environment        = "dev"

# Azure AD Groups (crear previamente)
data_engineers_group_name = "DataEngineers"
data_scientists_group_name = "DataScientists"

# ConfiguraciÃ³n de seguridad
enable_rbac_audit = true
enable_azure_policies = true
enable_mfa_for_admin = true

# Base de datos
sql_database_backup_retention_days = 7
sql_database_geo_redundant_backup = true
```

#### 3. **Desplegar infraestructura**

```bash
# Planificar despliegue
terraform plan

# Aplicar cambios
terraform apply

# Obtener outputs importantes
terraform output
```

#### 4. **Configurar secretos en Key Vault**

```bash
# Obtener nombre del Key Vault
KV_NAME=$(terraform output -raw key_vault_name)

# Configurar secretos
az keyvault secret set --vault-name $KV_NAME --name "sql-connection-string" --value "your-connection-string"
az keyvault secret set --vault-name $KV_NAME --name "storage-account-key" --value "your-storage-key"
az keyvault secret set --vault-name $KV_NAME --name "event-hub-connection-string" --value "your-eventhub-connection"
```

#### 5. **Ejecutar pipeline en Azure**

```bash
# Configurar variables para Azure
export ENVIRONMENT=azure
export AZURE_KEY_VAULT_NAME=$KV_NAME
export AZURE_STORAGE_ACCOUNT=$(terraform output -raw storage_account_name)

# Ejecutar pipeline
python pipelines/spark_job.py --config config/datasets/finanzas/payments_v1/pipeline.yml --env azure
```

### ğŸ” Monitoreo y observabilidad

#### Logs estructurados
```bash
# Ver logs en tiempo real
tail -f logs/pipeline.log

# Buscar por correlation ID
grep "correlation_id=abc123" logs/pipeline.log

# Ver mÃ©tricas
grep "metrics" logs/pipeline.log | jq .
```

#### Azure Monitor (en Azure)
- **Application Insights:** MÃ©tricas y trazas
- **Log Analytics:** Consultas KQL
- **Alertas:** Configuradas automÃ¡ticamente

---

## âš™ï¸ ConfiguraciÃ³n de pipelines

### ğŸ“‹ Esquema de datos

En `config/datasets/.../schema.json` se define cada columna con nombre, tipo y si es requerida:

```json
{
  "type": "object",
  "properties": {
    "payment_id": { "type": "string" },
    "amount": { "type": "number" },
    "payment_date": { "type": ["string", "null"], "format": "date-time" },
    "updated_at": { "type": ["string", "null"], "format": "date-time" }
  },
  "required": ["payment_id", "amount"]
}
```

### ğŸ”„ EstandarizaciÃ³n

En `pipeline.yml`:

```yaml
standardization:
  timezone: America/Bogota
  rename:
    - { from: customerId, to: customer_id }
  casts:
    - { column: amount, to: "decimal(18,2)", on_error: null }
    - { column: payment_date, to: "timestamp", format_hint: "yyyy-MM-dd[ HH:mm:ss]" }
  defaults:
    - { column: currency, value: "CLP" }
  deduplicate:
    key: [payment_id]
    order_by: [updated_at desc]
```

### âœ… Reglas de calidad

En `expectations.yml` se definen validaciones:

```yaml
expectations:
  - { column: amount, rule: ">= 0", action: quarantine }
  - { column: payment_date, rule: "not null", action: reject }
```

---

## ğŸ”’ Seguridad

> ğŸ“– **DocumentaciÃ³n completa:** Ver [docs/SECURITY.md](docs/SECURITY.md)

### ğŸ›¡ï¸ CaracterÃ­sticas de seguridad implementadas

- **ğŸ” GestiÃ³n de secretos:** Azure Key Vault con RBAC
- **ğŸ”’ Cifrado:** Datos en reposo y en trÃ¡nsito (TLS 1.2+)
- **ğŸ‘¥ Control de acceso:** Azure AD con grupos y roles personalizados
- **ğŸ“Š AuditorÃ­a:** Logs estructurados con correlaciÃ³n
- **ğŸš« ProtecciÃ³n:** WAF + NSG + Private Endpoints
- **ğŸ”‘ AutenticaciÃ³n:** JWT tokens con expiraciÃ³n
- **ğŸ­ Enmascaramiento:** Datos sensibles (PII)

### ğŸ”§ ConfiguraciÃ³n de seguridad

```python
from src.utils.security import get_encryption_manager, require_permission

# Cifrado de datos sensibles
encryption_manager = get_encryption_manager()
encrypted_data = encryption_manager.encrypt_field("sensitive_value", "credit_card")

# Control de acceso con decoradores
@require_permission("data.read")
def read_sensitive_data():
    return "sensitive data"
```

---

## ğŸ§ª Pruebas y CI/CD

### ğŸ”¬ Ejecutar pruebas

```bash
# Todas las pruebas
python -m pytest tests/ -v

# Pruebas especÃ­ficas
python -m pytest tests/test_security.py -v
python -m pytest tests/test_azure_integration.py -v

# Con cobertura
python -m pytest tests/ --cov=src --cov-report=html
```

### ğŸ” ValidaciÃ³n de configuraciones

```bash
# Validar configuraciones YAML/JSON
./ci/check_config.sh

# Lint de cÃ³digo
python -m flake8 src/
python -m black src/ --check

# ValidaciÃ³n de seguridad
python -m bandit -r src/
```

### ğŸš€ CI/CD Pipeline

- **`ci/lint.yml`:** ValidaciÃ³n de cÃ³digo y configuraciones
- **`ci/test_dataset.yml`:** Pruebas con datasets mÃ­nimos
- **GitHub Actions:** EjecuciÃ³n automÃ¡tica en PRs

---

## ğŸ“š DocumentaciÃ³n

### ğŸ“– GuÃ­as disponibles

- **[SECURITY.md](docs/SECURITY.md):** GuÃ­a completa de seguridad
- **[DEPLOYMENT.md](docs/DEPLOYMENT.md):** Instrucciones de despliegue
- **[API Documentation](src/):** DocumentaciÃ³n del cÃ³digo

### ğŸ¯ Buenas prÃ¡cticas

#### ğŸ”§ ConfiguraciÃ³n
- Usar rutas S3 (`s3a://raw/...`) en lugar de rutas locales
- Mantener actualizado `schema.json` al cambiar columnas
- Agregar reglas de calidad en `expectations.yml`
- Versionar datasets (`payments_v1`, `payments_v2`)

#### ğŸ”’ Seguridad
- Nunca hardcodear secretos en el cÃ³digo
- Usar Azure Key Vault para gestiÃ³n de secretos
- Aplicar principio de menor privilegio
- Auditar todos los accesos a datos sensibles

#### ğŸ“Š Observabilidad
- Usar correlation IDs en todos los logs
- Implementar mÃ©tricas de calidad de datos
- Configurar alertas para errores crÃ­ticos
- Monitorear rendimiento de pipelines

---

## ğŸ¤ Contribuciones

### ğŸ”„ Proceso de desarrollo

1. **Crear rama:**
```bash
git checkout -b feature/nueva-funcionalidad
```

2. **Desarrollar:**
```bash
# Hacer cambios en cÃ³digo/configuraciÃ³n
# Agregar pruebas unitarias
# Actualizar documentaciÃ³n si es necesario
```

3. **Validar:**
```bash
# Ejecutar pruebas
python -m pytest tests/ -v

# Validar configuraciones
./ci/check_config.sh

# Probar localmente
make run
```

4. **Crear PR:**
```bash
git push origin feature/nueva-funcionalidad
# Crear Pull Request en GitHub
```

### ğŸ“‹ Checklist para PRs

- [ ] âœ… Pruebas unitarias agregadas/actualizadas
- [ ] ğŸ”’ RevisiÃ³n de seguridad completada
- [ ] ğŸ“– DocumentaciÃ³n actualizada
- [ ] ğŸ§ª Pruebas locales exitosas
- [ ] ğŸ” ValidaciÃ³n de configuraciones
- [ ] ğŸ“Š Logs estructurados implementados

---

## ğŸ“ Soporte

### ğŸ†˜ ResoluciÃ³n de problemas

1. **Revisar logs:** `tail -f logs/pipeline.log`
2. **Verificar configuraciÃ³n:** `./ci/check_config.sh`
3. **Consultar documentaciÃ³n:** [docs/](docs/)
4. **Crear issue:** GitHub Issues

### ğŸ“§ Contacto

- **Equipo de Data Engineering:** data-engineering@company.com
- **Soporte tÃ©cnico:** tech-support@company.com
- **Seguridad:** security@company.com
