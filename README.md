# MVP Config-Driven Data Pipeline

Este proyecto implementa un **pipeline de datos din√°mico y flexible** basado en **Apache Spark + PostgreSQL**, dise√±ado para procesar datos a trav√©s de m√∫ltiples capas (Source ‚Üí Silver ‚Üí Gold) con configuraci√≥n completamente externalizada.

## üéØ Caracter√≠sticas Principales

- ‚úÖ **Pipeline Multi-Capa**: Source ‚Üí Silver (Parquet) ‚Üí Gold (PostgreSQL)
- ‚úÖ **Configuraci√≥n Externalizada**: Esquemas JSON, reglas de calidad YAML
- ‚úÖ **Validaci√≥n de Calidad**: Reglas configurables con cuarentena autom√°tica
- ‚úÖ **Esquemas Din√°micos**: Creaci√≥n autom√°tica de tablas desde JSON Schema
- ‚úÖ **Particionado Inteligente**: Por fecha con columnas autom√°ticas
- ‚úÖ **Transformaciones Configurables**: Renombrado, casting, valores por defecto
- ‚úÖ **Deduplicaci√≥n**: Basada en claves y ordenamiento configurables
- ‚úÖ **Containerizado**: Docker Compose para desarrollo y producci√≥n

## üèóÔ∏è Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Source    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Silver    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Gold     ‚îÇ
‚îÇ   (CSV)     ‚îÇ    ‚îÇ  (Parquet)   ‚îÇ    ‚îÇ(PostgreSQL) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                   ‚îÇ                   ‚îÇ
       ‚ñº                   ‚ñº                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Validaci√≥n  ‚îÇ    ‚îÇParticionado  ‚îÇ    ‚îÇ Esquemas    ‚îÇ
‚îÇ de Calidad  ‚îÇ    ‚îÇpor Fecha     ‚îÇ    ‚îÇ Din√°micos   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

```
mvp-config-driven/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ env.yml                    # Variables de entorno
‚îÇ   ‚îú‚îÄ‚îÄ database.yml               # Configuraci√≥n de PostgreSQL
‚îÇ   ‚îî‚îÄ‚îÄ datasets/                  # Configuraciones de datasets
‚îÇ       ‚îî‚îÄ‚îÄ finanzas/
‚îÇ           ‚îú‚îÄ‚îÄ payments_v1/       # Dataset payments versi√≥n 1
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ dataset.yml    # Configuraci√≥n del pipeline
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ dataset_with_gold.yml  # Configuraci√≥n con BD
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ schema.json    # Esquema JSON del dataset
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ expectations.yml # Reglas de calidad
‚îÇ           ‚îî‚îÄ‚îÄ payments_v2/       # Dataset payments versi√≥n 2
‚îú‚îÄ‚îÄ data/                          # Datos de entrada
‚îÇ   ‚îî‚îÄ‚îÄ raw/
‚îÇ       ‚îî‚îÄ‚îÄ payments/              # Archivos CSV de pagos
‚îÇ   ‚îî‚îÄ‚îÄ sample_payments.csv        # Datos de ejemplo
‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îú‚îÄ‚îÄ spark_job_with_db.py       # Pipeline principal
‚îÇ   ‚îú‚îÄ‚îÄ db_manager.py              # Gestor de base de datos
‚îÇ   ‚îî‚îÄ‚îÄ schema_mapper.py           # Mapeo de esquemas
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ run_pipeline.py            # Script de ejecuci√≥n
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ MANUAL_TESTING_GUIDE.md    # Gu√≠a de pruebas manuales
‚îÇ   ‚îî‚îÄ‚îÄ PROJECT_STRUCTURE.md       # Documentaci√≥n del proyecto
‚îî‚îÄ‚îÄ docker-compose.yml             # Servicios Docker
```

## üöÄ Inicio R√°pido

### Prerrequisitos

- **Docker** y **Docker Compose**
- **Python 3.8+** con PySpark
- **DBeaver** (opcional, para visualizaci√≥n)

### 1. Levantar la Infraestructura

```bash
# Iniciar PostgreSQL
docker-compose up -d

# Verificar que est√© funcionando
docker-compose ps
```

### 2. Ejecutar el Pipeline

```bash
# Ejecutar pipeline completo
python pipelines/spark_job_with_db.py config/datasets/finanzas/payments_v1/dataset_with_gold.yml config/env.yml config/database.yml development

```bash
docker-compose exec runner python pipelines/spark_job_with_db.py config/datasets/finanzas/payments_v1/dataset_with_gold.yml config/env.yml config/database.yml development
```
### 3. Verificar Resultados

```bash
# Conectar a PostgreSQL y verificar datos
docker exec -it mvp-postgres psql -U testuser -d testdb -c "SELECT * FROM test_payments_v1 LIMIT 5;"
```

## üìã Documentaci√≥n Completa

- **[Gu√≠a de Pruebas Manuales](docs/MANUAL_TESTING_GUIDE.md)**: Instrucciones paso a paso para probar el pipeline
- **[Estructura del Proyecto](docs/PROJECT_STRUCTURE.md)**: Documentaci√≥n detallada de la arquitectura

## üîß Configuraci√≥n

### Variables de Entorno (`config/env.yml`)

```yaml
spark:
  app_name: "ConfigDrivenPipeline"
  master: "local[*]"
  
paths:
  silver_base: "./silver_data"
  quarantine_base: "./quarantine_data"
```

### Configuraci√≥n de Base de Datos (`config/database.yml`)

```yaml
postgresql:
  host: localhost
  port: 5432
  database: testdb
  username: testuser
  password: testpass
```

## üéØ Casos de Uso

### Agregar Nuevo Dataset

1. Crear directorio en `config/datasets/[categoria]/[dataset_name]/`
2. Crear esquema JSON en `schema.json`
3. Definir reglas de calidad en `expectations.yml`
4. Configurar pipeline en `dataset.yml` o `dataset_with_gold.yml`
5. Ejecutar pipeline

### Modificar Transformaciones

1. Actualizar archivo de configuraci√≥n del dataset con nuevas reglas
2. Ejecutar pipeline sin cambios de c√≥digo

### Conectar Nueva Base de Datos

1. Actualizar `config/database.yml`
2. Modificar `db_manager.py` si es necesario

## üõ†Ô∏è Desarrollo y Extensi√≥n

### Agregar Nuevas Transformaciones

Editar archivo de configuraci√≥n del dataset (ej. `config/datasets/finanzas/payments_v1/dataset.yml`):

```yaml
standardization:
  rename:
    - { from: "old_column", to: "new_column" }
  casts:
    - { column: "amount", to: "decimal(18,2)", on_error: "null" }
  defaults:
    - { column: "currency", value: "USD" }
```

### Modificar Reglas de Calidad

Editar archivo de expectations del dataset (ej. `config/datasets/finanzas/payments_v1/expectations.yml`):

```yaml
expectations:
  - column: "amount"
    rule: ">= 0"
    action: "quarantine"
  - column: "payment_id"
    rule: "not null"
    action: "reject"
```

### Cambiar Esquema de Base de Datos

1. Actualizar <mcfile name="schema.json" path="test_data/schema.json"></mcfile>
2. El pipeline crear√° autom√°ticamente la tabla con el nuevo esquema

## üîç Monitoreo y Logs

### Ver Logs del Pipeline

```bash
# Logs de Spark
python pipelines/spark_job_with_db.py ... --verbose

# Logs de PostgreSQL
docker logs mvp-postgres
```

### Verificar Estado de la Base de Datos

```bash
# Conectar a PostgreSQL
docker exec -it mvp-postgres psql -U testuser -d testdb

# Ver tablas
\dt

# Ver datos
SELECT * FROM test_payments_v1 LIMIT 10;
```

## üö® Troubleshooting

### Problemas Comunes

#### 1. Error de Conexi√≥n a PostgreSQL

```bash
# Verificar que el contenedor est√© ejecut√°ndose
docker-compose ps

# Reiniciar servicios
docker-compose down && docker-compose up -d
```

#### 2. Error de Esquema JSON

- Verificar que el archivo `schema.json` del dataset tenga formato v√°lido
- Usar herramientas online para validar JSON

#### 3. Datos No Aparecen en la Base de Datos

- Verificar que el archivo CSV en `data/raw/` tenga datos v√°lidos
- Revisar logs del pipeline para errores de calidad
- Verificar que las reglas de `expectations.yml` del dataset no rechacen todos los datos

## üìà Pr√≥ximos Pasos

- [ ] **Orquestaci√≥n**: Integraci√≥n con Apache Airflow
- [ ] **Streaming**: Soporte para Apache Kafka
- [ ] **Monitoreo**: Dashboard con m√©tricas del pipeline
- [ ] **Alertas**: Notificaciones autom√°ticas en caso de fallos
- [ ] **Escalabilidad**: Despliegue en Kubernetes

## ü§ù Contribuciones

1. Fork del repositorio
2. Crear rama feature: `git checkout -b feature/nueva-funcionalidad`
3. Commit cambios: `git commit -am 'Agregar nueva funcionalidad'`
4. Push a la rama: `git push origin feature/nueva-funcionalidad`
5. Crear Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Ver el archivo `LICENSE` para m√°s detalles.

---

**¬øNecesitas ayuda?** Consulta la [Gu√≠a de Pruebas Manuales](docs/MANUAL_TESTING_GUIDE.md) para instrucciones detalladas paso a paso.
