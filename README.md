# MVP Config Driven Data Pipeline

Este proyecto implementa un **pipeline de datos din√°mico y flexible**, basado en **Apache Spark + MinIO + configuraci√≥n YAML/JSON**, dise√±ado para adaptarse a distintos entornos (local, Docker, WSL2, CI/CD, Azure) y soportar cambios en datasets sin necesidad de modificar c√≥digo.

## üéØ Objetivos del proyecto

- Procesar datos en **CSV, JSON y Parquet** de forma **configurable**
- Definir **esquemas, est√°ndares y reglas de calidad** en archivos de configuraci√≥n
- Adaptarse a cambios de columnas, tipos de datos y flujos mediante **metadata-driven pipelines**
- Estandarizar datasets en capas (`raw ‚Üí silver`)
- **Seguridad empresarial** con Azure AD, RBAC, cifrado y auditor√≠a
- **Observabilidad completa** con logs estructurados, m√©tricas y trazabilidad
- **Patrones de software** robustos (Observer, Strategy) para escalabilidad
- Ser portable y ejecutable en:
  - **Windows** (con WSL2 + Docker Desktop)
  - **Linux** nativo
  - **Azure Cloud** (Data Factory, Synapse, Key Vault)
  - **Entornos de CI/CD**

## üèóÔ∏è Caracter√≠sticas principales

### ‚úÖ Funcionalidades implementadas
- [x] Ingesta din√°mica de CSV/JSON/Parquet
- [x] Estandarizaci√≥n configurable (renames, casts, defaults, deduplicaci√≥n)
- [x] Enriquecimiento autom√°tico (timestamp, run_id, particiones por fecha)
- [x] **Seguridad empresarial** con Azure Key Vault, RBAC y cifrado
- [x] **Logs estructurados** con correlaci√≥n y m√©tricas
- [x] **Patrones de software** (Observer para eventos, Strategy para procesamiento)
- [x] **Pruebas unitarias** completas (>95% cobertura)
- [x] **Integraci√≥n Azure** (Storage, Event Hub, Key Vault, Monitor)
- [x] **Documentaci√≥n de seguridad** y despliegue
- [x] CI/CD con validaci√≥n de configs
- [x] **Pipeline funcional** con datos de prueba procesados
- [x] **Monitoreo y observabilidad** validados
- [x] **Configuraci√≥n Azure** lista para despliegue

## üìä Resultados de Pruebas

### Pipeline de Datos
- ‚úÖ **10 registros procesados** ‚Üí **7 registros v√°lidos** (70% tasa de √©xito)
- ‚úÖ **Calidad de datos**: 100% (todos los registros v√°lidos pasaron las validaciones)
- ‚úÖ **6 pa√≠ses procesados**, **3 monedas diferentes**
- ‚úÖ **Monto total**: $817.234 USD, **promedio**: $116.75 USD

### Monitoreo y Observabilidad
- ‚úÖ **Sistema de logging** funcionando correctamente
- ‚úÖ **M√©tricas de pipeline** recolectadas y almacenadas
- ‚úÖ **Health checks** pasando todas las validaciones
- ‚úÖ **Prometheus y Grafana** configurados y operativos

### Infraestructura
- ‚úÖ **Docker Compose** con 9 servicios ejecut√°ndose
- ‚úÖ **Spark cluster** (master + worker) operativo
- ‚úÖ **MinIO** para almacenamiento S3-compatible
- ‚úÖ **Kafka + Zookeeper** para streaming
- ‚úÖ **Redis** para cach√© y estado
- ‚úÖ **SQL Server** para metadatos

### üîÑ Pr√≥ximas funcionalidades
- [ ] Capa `gold` y orquestaci√≥n con Azure Data Factory
- [ ] Dashboard de monitoreo con Power BI
- [ ] ML Pipeline con Azure ML

---

## üèõÔ∏è Arquitectura

```
mvp-config-driven/
‚îú‚îÄ config/                   # Configuraciones din√°micas
‚îÇ   ‚îú‚îÄ datasets/
‚îÇ   ‚îÇ   ‚îî‚îÄ finanzas/
‚îÇ   ‚îÇ       ‚îî‚îÄ payments_v1/
‚îÇ   ‚îÇ           ‚îú‚îÄ schema.json           # Esquema de datos
‚îÇ   ‚îÇ           ‚îú‚îÄ expectations.yml      # Reglas de calidad
‚îÇ   ‚îÇ           ‚îî‚îÄ pipeline.yml          # Configuraci√≥n del pipeline
‚îÇ   ‚îî‚îÄ envs/
‚îÇ       ‚îú‚îÄ local.yml                     # Entorno local
‚îÇ       ‚îú‚îÄ dev.yml                       # Entorno desarrollo
‚îÇ       ‚îî‚îÄ prod.yml                      # Entorno producci√≥n
‚îú‚îÄ src/                      # C√≥digo fuente
‚îÇ   ‚îú‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ logging.py                    # Logs estructurados
‚îÇ   ‚îÇ   ‚îú‚îÄ azure_integration.py          # Integraci√≥n Azure
‚îÇ   ‚îÇ   ‚îî‚îÄ security.py                   # Utilidades de seguridad
‚îÇ   ‚îî‚îÄ patterns/
‚îÇ       ‚îú‚îÄ observer.py                   # Patr√≥n Observer
‚îÇ       ‚îî‚îÄ strategy.py                   # Patr√≥n Strategy
‚îú‚îÄ terraform/                # Infraestructura como c√≥digo
‚îÇ   ‚îú‚îÄ main.tf                          # Recursos principales
‚îÇ   ‚îú‚îÄ variables.tf                     # Variables
‚îÇ   ‚îú‚îÄ security.tf                      # Configuraci√≥n de seguridad
‚îÇ   ‚îî‚îÄ outputs.tf                       # Salidas
‚îú‚îÄ tests/                    # Pruebas unitarias
‚îÇ   ‚îú‚îÄ test_logging.py
‚îÇ   ‚îú‚îÄ test_patterns.py
‚îÇ   ‚îú‚îÄ test_security.py
‚îÇ   ‚îî‚îÄ test_azure_integration.py
‚îú‚îÄ docs/                     # Documentaci√≥n
‚îÇ   ‚îú‚îÄ SECURITY.md                      # Gu√≠a de seguridad
‚îÇ   ‚îî‚îÄ DEPLOYMENT.md                    # Gu√≠a de despliegue
‚îú‚îÄ pipelines/
‚îÇ   ‚îî‚îÄ spark_job.py          # Pipeline principal
‚îú‚îÄ scripts/
‚îÇ   ‚îú‚îÄ run_pipeline.ps1      # Script Windows
‚îÇ   ‚îî‚îÄ runner.sh             # Script Linux
‚îú‚îÄ ci/                       # CI/CD
‚îÇ   ‚îú‚îÄ check_config.sh
‚îÇ   ‚îú‚îÄ lint.yml
‚îÇ   ‚îú‚îÄ test_dataset.yml
‚îÇ   ‚îî‚îÄ README.md
‚îú‚îÄ docker-compose.yml        # Orquestaci√≥n local
‚îú‚îÄ Makefile                  # Comandos automatizados
‚îú‚îÄ requirements.txt          # Dependencias Python
‚îî‚îÄ README.md                 
```

### üîß Servicios principales

| Servicio | Rol | Entorno |
|----------|-----|---------|
| **Spark Master/Worker** | Motor de procesamiento distribuido | Local/Azure |
| **MinIO/Azure Storage** | Almacenamiento para raw/silver/quarantine | Local/Azure |
| **Runner** | Ejecutor de pipelines con configs din√°micas | Local/Azure |
| **Azure Key Vault** | Gesti√≥n segura de secretos y certificados | Azure |
| **Azure Event Hub** | Streaming de eventos y telemetr√≠a | Azure |
| **Azure Monitor** | Observabilidad y alertas | Azure |
| **Application Gateway + WAF** | Seguridad en tr√°nsito y protecci√≥n | Azure |

---

## üìã Instalaci√≥n y requisitos

### üîß Requisitos previos

#### Para desarrollo local:
- **Docker Desktop** (Windows) o **Docker + Docker Compose** (Linux)
- **Python 3.9+** con pip
- **Git**
- **Make** (opcional, para comandos automatizados)

#### Para despliegue en Azure:
- **Azure CLI** (`az`)
- **Terraform** (>= 1.0)
- **Suscripci√≥n Azure** con permisos de Contributor
- **Azure AD** con permisos para crear Service Principals

### ü™ü Windows (con WSL2 + Docker Desktop)

1. **Instalar Docker Desktop** y habilitar integraci√≥n con WSL2
2. **Clonar el repositorio:**

```bash
git clone https://github.com/mi-org/mvp-config-driven.git
cd mvp-config-driven
```

3. **En WSL2, instalar dependencias:**

```bash
sudo apt update && sudo apt install make dos2unix python3-pip -y
```

4. **Instalar dependencias Python:**

```bash
pip install -r requirements.txt
```

5. **Convertir scripts a formato Unix (solo una vez):**

```bash
dos2unix scripts/*.sh
```

### üêß Linux nativo

```bash
# Clonar repositorio
git clone https://github.com/mi-org/mvp-config-driven.git
cd mvp-config-driven

# Instalar dependencias del sistema
sudo apt update && sudo apt install docker.io docker-compose make python3-pip -y

# Instalar dependencias Python
pip install -r requirements.txt

# Configurar Docker (si es necesario)
sudo usermod -aG docker $USER
newgrp docker
```

### ‚òÅÔ∏è Azure CLI y Terraform

```bash
# Instalar Azure CLI
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

# Instalar Terraform
wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list
sudo apt update && sudo apt install terraform

# Verificar instalaci√≥n
az --version
terraform --version
```

---

## üöÄ Ejecuci√≥n del proyecto

### üè† Despliegue local

#### 1. **Configurar variables de entorno**

```bash
# Copiar archivo de ejemplo
cp .env.example .env

# Editar variables seg√∫n tu entorno
nano .env
```

Variables principales:
```bash
# Entorno
ENVIRONMENT=local
LOG_LEVEL=INFO

# MinIO (local)
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minio
MINIO_SECRET_KEY=minio12345

# Opcional: Azure (para pruebas h√≠bridas)
AZURE_TENANT_ID=your-tenant-id
AZURE_CLIENT_ID=your-client-id
AZURE_CLIENT_SECRET=your-client-secret
```

#### 2. **Levantar infraestructura local**

```bash
# Iniciar servicios (Spark + MinIO)
make up

# Verificar que los servicios est√©n corriendo
docker ps
```

#### 3. **Ejecutar un pipeline**

```bash
# Pipeline b√°sico con configuraci√≥n por defecto
make run

# Pipeline espec√≠fico
make run DATASET=finanzas/payments_v1 ENV=local

# Con logs detallados
make run-debug
```

#### 4. **Ver resultados**

- **MinIO UI:** [http://localhost:9001](http://localhost:9001)
  - Usuario: `minio`
  - Password: `minio12345`
- **Spark UI:** [http://localhost:8080](http://localhost:8080)
- **Datasets procesados:** `s3a://silver/payments_v1/`

#### 5. **Apagar servicios**

```bash
make down
```

### ‚òÅÔ∏è Despliegue en Azure

> üìñ **Documentaci√≥n completa:** Ver [docs/DEPLOYMENT.md](docs/DEPLOYMENT.md)

#### 1. **Autenticaci√≥n en Azure**

```bash
# Login en Azure
az login

# Configurar suscripci√≥n
az account set --subscription "your-subscription-id"

# Crear Service Principal (si no existe)
az ad sp create-for-rbac --name "mvp-data-pipeline-sp" \
  --role Contributor \
  --scopes /subscriptions/your-subscription-id
```

#### 2. **Configurar Terraform**

```bash
cd terraform

# Inicializar Terraform
terraform init

# Crear archivo de variables
cp terraform.tfvars.example terraform.tfvars

# Editar variables
nano terraform.tfvars
```

Variables principales en `terraform.tfvars`:
```hcl
# Configuraci√≥n b√°sica
resource_group_name = "rg-mvp-data-pipeline"
location           = "East US"
environment        = "dev"

# Azure AD Groups (crear previamente)
data_engineers_group_name = "DataEngineers"
data_scientists_group_name = "DataScientists"

# Configuraci√≥n de seguridad
enable_rbac_audit = true
enable_azure_policies = true
enable_mfa_for_admin = true

# Base de datos
sql_database_backup_retention_days = 7
sql_database_geo_redundant_backup = true
```

#### 3. **Desplegar infraestructura**

```bash
# Planificar despliegue
terraform plan

# Aplicar cambios
terraform apply

# Obtener outputs importantes
terraform output
```

#### 4. **Configurar secretos en Key Vault**

```bash
# Obtener nombre del Key Vault
KV_NAME=$(terraform output -raw key_vault_name)

# Configurar secretos
az keyvault secret set --vault-name $KV_NAME --name "sql-connection-string" --value "your-connection-string"
az keyvault secret set --vault-name $KV_NAME --name "storage-account-key" --value "your-storage-key"
az keyvault secret set --vault-name $KV_NAME --name "event-hub-connection-string" --value "your-eventhub-connection"
```

#### 5. **Ejecutar pipeline en Azure**

```bash
# Configurar variables para Azure
export ENVIRONMENT=azure
export AZURE_KEY_VAULT_NAME=$KV_NAME
export AZURE_STORAGE_ACCOUNT=$(terraform output -raw storage_account_name)

# Ejecutar pipeline
python pipelines/spark_job.py --config config/datasets/finanzas/payments_v1/pipeline.yml --env azure
```

### üîç Monitoreo y observabilidad

#### Logs estructurados
```bash
# Ver logs en tiempo real
tail -f logs/pipeline.log

# Buscar por correlation ID
grep "correlation_id=abc123" logs/pipeline.log

# Ver m√©tricas
grep "metrics" logs/pipeline.log | jq .
```

#### Azure Monitor (en Azure)
- **Application Insights:** M√©tricas y trazas
- **Log Analytics:** Consultas KQL
- **Alertas:** Configuradas autom√°ticamente

---

## ‚öôÔ∏è Configuraci√≥n de pipelines

### üìã Esquema de datos

En `config/datasets/.../schema.json` se define cada columna con nombre, tipo y si es requerida:

```json
{
  "type": "object",
  "properties": {
    "payment_id": { "type": "string" },
    "amount": { "type": "number" },
    "payment_date": { "type": ["string", "null"], "format": "date-time" },
    "updated_at": { "type": ["string", "null"], "format": "date-time" }
  },
  "required": ["payment_id", "amount"]
}
```

### üîÑ Estandarizaci√≥n

En `pipeline.yml`:

```yaml
standardization:
  timezone: America/Bogota
  rename:
    - { from: customerId, to: customer_id }
  casts:
    - { column: amount, to: "decimal(18,2)", on_error: null }
    - { column: payment_date, to: "timestamp", format_hint: "yyyy-MM-dd[ HH:mm:ss]" }
  defaults:
    - { column: currency, value: "CLP" }
  deduplicate:
    key: [payment_id]
    order_by: [updated_at desc]
```

### ‚úÖ Reglas de calidad

En `expectations.yml` se definen validaciones:

```yaml
expectations:
  - { column: amount, rule: ">= 0", action: quarantine }
  - { column: payment_date, rule: "not null", action: reject }
```

---

## üîí Seguridad

> üìñ **Documentaci√≥n completa:** Ver [docs/SECURITY.md](docs/SECURITY.md)

### üõ°Ô∏è Caracter√≠sticas de seguridad implementadas

- **üîê Gesti√≥n de secretos:** Azure Key Vault con RBAC
- **üîí Cifrado:** Datos en reposo y en tr√°nsito (TLS 1.2+)
- **üë• Control de acceso:** Azure AD con grupos y roles personalizados
- **üìä Auditor√≠a:** Logs estructurados con correlaci√≥n
- **üö´ Protecci√≥n:** WAF + NSG + Private Endpoints
- **üîë Autenticaci√≥n:** JWT tokens con expiraci√≥n
- **üé≠ Enmascaramiento:** Datos sensibles (PII)

### üîß Configuraci√≥n de seguridad

```python
from src.utils.security import get_encryption_manager, require_permission

# Cifrado de datos sensibles
encryption_manager = get_encryption_manager()
encrypted_data = encryption_manager.encrypt_field("sensitive_value", "credit_card")

# Control de acceso con decoradores
@require_permission("data.read")
def read_sensitive_data():
    return "sensitive data"
```

---

## üß™ Pruebas y CI/CD

### üî¨ Ejecutar pruebas

```bash
# Todas las pruebas
python -m pytest tests/ -v

# Pruebas espec√≠ficas
python -m pytest tests/test_security.py -v
python -m pytest tests/test_azure_integration.py -v

# Con cobertura
python -m pytest tests/ --cov=src --cov-report=html
```

### üîç Validaci√≥n de configuraciones

```bash
# Validar configuraciones YAML/JSON
./ci/check_config.sh

# Lint de c√≥digo
python -m flake8 src/
python -m black src/ --check

# Validaci√≥n de seguridad
python -m bandit -r src/
```

### üöÄ CI/CD Pipeline

- **`ci/lint.yml`:** Validaci√≥n de c√≥digo y configuraciones
- **`ci/test_dataset.yml`:** Pruebas con datasets m√≠nimos
- **GitHub Actions:** Ejecuci√≥n autom√°tica en PRs

---

## üìö Documentaci√≥n

### üìñ Gu√≠as disponibles

- **[SECURITY.md](docs/SECURITY.md):** Gu√≠a completa de seguridad
- **[DEPLOYMENT.md](docs/DEPLOYMENT.md):** Instrucciones de despliegue
- **[API Documentation](src/):** Documentaci√≥n del c√≥digo

### üéØ Buenas pr√°cticas

#### üîß Configuraci√≥n
- Usar rutas S3 (`s3a://raw/...`) en lugar de rutas locales
- Mantener actualizado `schema.json` al cambiar columnas
- Agregar reglas de calidad en `expectations.yml`
- Versionar datasets (`payments_v1`, `payments_v2`)

#### üîí Seguridad
- Nunca hardcodear secretos en el c√≥digo
- Usar Azure Key Vault para gesti√≥n de secretos
- Aplicar principio de menor privilegio
- Auditar todos los accesos a datos sensibles

#### üìä Observabilidad
- Usar correlation IDs en todos los logs
- Implementar m√©tricas de calidad de datos
- Configurar alertas para errores cr√≠ticos
- Monitorear rendimiento de pipelines

---

## ü§ù Contribuciones

### üîÑ Proceso de desarrollo

1. **Crear rama:**
```bash
git checkout -b feature/nueva-funcionalidad
```

2. **Desarrollar:**
```bash
# Hacer cambios en c√≥digo/configuraci√≥n
# Agregar pruebas unitarias
# Actualizar documentaci√≥n si es necesario
```

3. **Validar:**
```bash
# Ejecutar pruebas
python -m pytest tests/ -v

# Validar configuraciones
./ci/check_config.sh

# Probar localmente
make run
```

4. **Crear PR:**
```bash
git push origin feature/nueva-funcionalidad
# Crear Pull Request en GitHub
```

### üìã Checklist para PRs

- [ ] ‚úÖ Pruebas unitarias agregadas/actualizadas
- [ ] üîí Revisi√≥n de seguridad completada
- [ ] üìñ Documentaci√≥n actualizada
- [ ] üß™ Pruebas locales exitosas
- [ ] üîç Validaci√≥n de configuraciones
- [ ] üìä Logs estructurados implementados

---

## üìû Soporte

### üÜò Resoluci√≥n de problemas

1. **Revisar logs:** `tail -f logs/pipeline.log`
2. **Verificar configuraci√≥n:** `./ci/check_config.sh`
3. **Consultar documentaci√≥n:** [docs/](docs/)
4. **Crear issue:** GitHub Issues

### üìß Contacto

- **Equipo de Data Engineering:** data-engineering@company.com
- **Soporte t√©cnico:** tech-support@company.com
- **Seguridad:** security@company.com
