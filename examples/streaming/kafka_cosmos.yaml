project: retail360
environment: dev
platform: azure
spark:
  shuffle_partitions: 2
datasets:
  - name: orders_stream
    layer: bronze
    source:
      type: kafka
      options:
        subscribe: retail-orders
        startingOffsets: earliest
        format: json
        kafka.bootstrap.servers: kafka:9092
        failOnDataLoss: false
      watermark_column: event_time
    transform:
      add_ingestion_ts: true
      ops:
        - flatten_json:
            depth: 1
        - cast:
            event_time: timestamp
            total_amount: decimal(18,2)
        - uppercase: [currency]
    validation:
      expect_not_null: [order_id, event_time]
      expect_between:
        col: total_amount
        min: 0
    streaming:
      enabled: true
      trigger: processingTime=1 minute
      checkpoint: abfs://checkpoints/retail/orders_stream/
      watermark_column: event_time
    sink:
      type: nosql
      engine: cosmosdb
      options:
        endpoint: ${SECRET:COSMOS_URI}
        masterkey: ${SECRET:COSMOS_KEY}
        database: retail
        collection: orders_stream
        checkpointLocation: abfs://checkpoints/retail/orders_stream/
        writeThroughput: 400
    incremental:
      mode: append
