project: retail360
environment: dev
platform: azure
spark:
  shuffle_partitions: 4
datasets:
  - name: orders_silver
    layer: silver
    source:
      - type: storage
        format: csv
        uri: abfs://landing/retail/orders/
        infer_schema: true
        options:
          header: true
          delimiter: ";"
      - type: endpoint
        url: https://api.retail.example.com/v1/orders
        method: GET
        auth:
          type: bearer
          token: ${SECRET:RETAIL_API_TOKEN}
        pagination:
          strategy: cursor
          param: cursor
          cursor_path: $.next
        record_path: data
        flatten: true
    merge_strategy:
      keys: [order_id]
      prefer: newest
      order_by: ["_ingestion_ts DESC"]
    transform:
      add_ingestion_ts: true
      ops:
        - trim: [customer_name, country]
        - uppercase: [country]
        - rename:
            country: country_code
        - cast:
            total_amount: decimal(18,2)
        - normalize_whitespace: [customer_name]
        - standardize_dates:
            cols: [order_date]
            format_in: "yyyy-MM-dd'T'HH:mm:ss'Z'"
            format_out: "yyyy-MM-dd"
            tz: UTC
        - deduplicate:
            keys: [order_id]
            order_by: ["order_date DESC", "_ingestion_ts DESC"]
    validation:
      rules:
        - check: expect_not_null
          columns: [order_id, order_date]
        - check: expect_unique
          columns: [order_id]
        - check: range
          column: total_amount
          min: 0
        - check: regex
          column: customer_email
          pattern: "^[^@\\s]+@[^@\\s]+\\.[^@\\s]+$"
          severity: warn
      quarantine_sink:
        type: storage
        uri: abfs://silver/retail/orders_silver/_quarantine/
    incremental:
      mode: merge
      keys: [order_id]
      order_by: ["order_date DESC", "_ingestion_ts DESC"]
      watermark_column: order_date
    sink:
      type: storage
      format: parquet
      uri: abfs://silver/retail/orders_silver/
      partition_by: [order_date]
      options:
        mergeSchema: true
        compression: snappy
      write_options:
        maxRecordsPerFile: 134217728
  - name: orders_gold
    layer: gold
    source:
      type: storage
      format: parquet
      uri: abfs://silver/retail/orders_silver/
      options:
        mergeSchema: true
    transform:
      sql:
        - |
          SELECT
            order_id,
            customer_name,
            customer_email,
            country_code,
            total_amount,
            CAST(order_date AS DATE) AS order_date,
            _ingestion_ts
          FROM orders_silver
    validation:
      expect_not_null: [order_id, order_date, customer_email]
      expect_regex:
        col: country_code
        pattern: "^[A-Z]{2}$"
      expect_length:
        col: customer_name
        min: 3
    incremental:
      mode: merge
      keys: [order_id]
      order_by: ["order_date DESC", "_ingestion_ts DESC"]
    sink:
      type: warehouse
      engine: synapse
      table: analytics.orders_gold
      partition_by: [order_date]
      options:
        batchsize: 10000
        truncate: false
        isolationLevel: READ_COMMITTED
        createTableOptions: "DISTRIBUTION = HASH(order_id)"
