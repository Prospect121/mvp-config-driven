id: events_multiformat

source:
  input_format: json  # Formato principal, pero el pipeline soporta CSV y Parquet
  path: "s3a://raw/casos-uso/events-multiformat/*.json"
  options:
    multiLine: "true"
    allowComments: "true"
    allowUnquotedFieldNames: "false"
    allowSingleQuotes: "false"
    allowNumericLeadingZeros: "false"
    allowBackslashEscapingAnyCharacter: "false"
    mode: "PERMISSIVE"  # Para capturar errores de parsing
    columnNameOfCorruptRecord: "_corrupt_record"

standardization:
  timezone: UTC
  
  rename:
    - { from: "id", to: event_id }
    - { from: "timestamp", to: event_timestamp }
    # user_id y event_type ya tienen los nombres correctos
    # Mapear campos anidados del metadata
    - { from: "metadata.session_id", to: session_id }
    # Campos que no existen en los datos generados se manejarÃ¡n con defaults
  
  casts:
    - { column: event_timestamp, to: "timestamp", format_hint: "yyyy-MM-dd[ HH:mm:ss]", on_error: null }
    - { column: user_id, to: "string", on_error: null }
  
  defaults:
    - { column: session_id, value: "" }
    - { column: device_info, value: "{}" }
    - { column: location_data, value: "{}" }
    - { column: custom_properties, value: "{}" }
    - { column: experiment_id, value: 0 }
    - { column: variant_id, value: 0 }
  
  deduplicate:
    key: [event_id, user_id, session_id]
    order_by: ["event_timestamp desc", "experiment_id desc"]

quality:
  expectations_ref: config/datasets/casos_uso/events_multiformat_expectations.yml
  quarantine: s3a://raw/quarantine/events-multiformat/

schema:
  ref: config/datasets/casos_uso/events_multiformat_schema.json
  mode: strict

output:
  silver:
    format: parquet
    path: "s3a://silver/events-multiformat/"
    # partition_by: [year, month]
    merge_schema: true
    mode: overwrite_dynamic
    # partition_from: event_timestamp
    
    # Optimizaciones para datos complejos
    options:
      "spark.sql.adaptive.enabled": "true"
      "spark.sql.adaptive.coalescePartitions.enabled": "true"
      "spark.sql.adaptive.skewJoin.enabled": "true"
      "spark.sql.adaptive.advisoryPartitionSizeInBytes": "128MB"
      "spark.serializer": "org.apache.spark.serializer.KryoSerializer"

  gold:
    enabled: true
    database_config: "config/database.yml"
    environment: "development"
    
    exclude_columns: ["_run_id", "_ingestion_ts", "year", "month", "day", "_corrupt_record"]
    
    # add_columns:
    #   - { name: "data_source", value: "multiformat_events", type: "string" }
    #   - { name: "processed_at", value: "current_timestamp()", type: "timestamp" }
    #   - { name: "batch_id", value: "uuid()", type: "string" }
    #   - { name: "pipeline_version", value: "v2.1.0", type: "string" }
    #   - { name: "data_quality_score", value: "CASE WHEN _corrupt_record IS NULL THEN 100.0 ELSE 0.0 END", type: "double" }
    #   - { name: "event_hour", value: "hour(event_timestamp)", type: "integer" }
    #   - { name: "is_weekend", value: "dayofweek(event_timestamp) IN (1, 7)", type: "boolean" }
    
    # business_rules:
    #   - { condition: "event_id IS NOT NULL", action: "filter" }
    #   - { condition: "user_id IS NOT NULL AND user_id > 0", action: "filter" }
    #   - { condition: "event_timestamp IS NOT NULL", action: "filter" }
    #   - { condition: "event_timestamp >= '2023-01-01'", action: "filter" }
    #   - { condition: "event_timestamp <= current_timestamp()", action: "filter" }
    #   - { condition: "event_type IN ('PAGE_VIEW', 'CLICK', 'PURCHASE', 'SIGNUP', 'LOGIN', 'LOGOUT', 'SEARCH', 'ADD_TO_CART', 'CHECKOUT', 'PAYMENT', 'ERROR', 'CUSTOM')", action: "filter" }
    #   # - { condition: "_corrupt_record IS NULL", action: "filter" }  # Only exists when there are JSON parsing errors
    #   - { condition: "LENGTH(event_id) <= 100", action: "filter" }
    #   - { condition: "session_id IS NULL OR LENGTH(session_id) <= 100", action: "filter" }